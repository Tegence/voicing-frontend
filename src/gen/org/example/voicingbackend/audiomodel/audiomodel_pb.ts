// @generated by protoc-gen-es v2.10.2 with parameter "target=ts,import_extension=ts"
// @generated from file org/example/voicingbackend/audiomodel/audiomodel.proto (package org.example.voicingbackend.audiomodel, syntax proto3)
/* eslint-disable */

import type { GenEnum, GenFile, GenMessage, GenService } from "@bufbuild/protobuf/codegenv2";
import { enumDesc, fileDesc, messageDesc, serviceDesc } from "@bufbuild/protobuf/codegenv2";
import type { Message } from "@bufbuild/protobuf";

/**
 * Describes the file org/example/voicingbackend/audiomodel/audiomodel.proto.
 */
export const file_org_example_voicingbackend_audiomodel_audiomodel: GenFile = /*@__PURE__*/
  fileDesc("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");

/**
 * User registration request
 *
 * @generated from message org.example.voicingbackend.audiomodel.RegisterUserRequest
 */
export type RegisterUserRequest = Message<"org.example.voicingbackend.audiomodel.RegisterUserRequest"> & {
  /**
   * @generated from field: string username = 1;
   */
  username: string;

  /**
   * @generated from field: string email = 2;
   */
  email: string;

  /**
   * @generated from field: string password = 3;
   */
  password: string;

  /**
   * @generated from field: string full_name = 4;
   */
  fullName: string;
};

/**
 * Describes the message org.example.voicingbackend.audiomodel.RegisterUserRequest.
 * Use `create(RegisterUserRequestSchema)` to create a new message.
 */
export const RegisterUserRequestSchema: GenMessage<RegisterUserRequest> = /*@__PURE__*/
  messageDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 0);

/**
 * User registration response
 *
 * @generated from message org.example.voicingbackend.audiomodel.RegisterUserResponse
 */
export type RegisterUserResponse = Message<"org.example.voicingbackend.audiomodel.RegisterUserResponse"> & {
  /**
   * @generated from field: bool success = 1;
   */
  success: boolean;

  /**
   * @generated from field: string message = 2;
   */
  message: string;

  /**
   * @generated from field: string user_id = 3;
   */
  userId: string;

  /**
   * @generated from field: string token = 4;
   */
  token: string;
};

/**
 * Describes the message org.example.voicingbackend.audiomodel.RegisterUserResponse.
 * Use `create(RegisterUserResponseSchema)` to create a new message.
 */
export const RegisterUserResponseSchema: GenMessage<RegisterUserResponse> = /*@__PURE__*/
  messageDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 1);

/**
 * User login request
 *
 * @generated from message org.example.voicingbackend.audiomodel.LoginUserRequest
 */
export type LoginUserRequest = Message<"org.example.voicingbackend.audiomodel.LoginUserRequest"> & {
  /**
   * @generated from field: string username = 1;
   */
  username: string;

  /**
   * @generated from field: string password = 2;
   */
  password: string;
};

/**
 * Describes the message org.example.voicingbackend.audiomodel.LoginUserRequest.
 * Use `create(LoginUserRequestSchema)` to create a new message.
 */
export const LoginUserRequestSchema: GenMessage<LoginUserRequest> = /*@__PURE__*/
  messageDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 2);

/**
 * User login response
 *
 * @generated from message org.example.voicingbackend.audiomodel.LoginUserResponse
 */
export type LoginUserResponse = Message<"org.example.voicingbackend.audiomodel.LoginUserResponse"> & {
  /**
   * @generated from field: bool success = 1;
   */
  success: boolean;

  /**
   * @generated from field: string message = 2;
   */
  message: string;

  /**
   * @generated from field: string user_id = 3;
   */
  userId: string;

  /**
   * @generated from field: string token = 4;
   */
  token: string;

  /**
   * @generated from field: org.example.voicingbackend.audiomodel.UserInfo user_info = 5;
   */
  userInfo?: UserInfo;
};

/**
 * Describes the message org.example.voicingbackend.audiomodel.LoginUserResponse.
 * Use `create(LoginUserResponseSchema)` to create a new message.
 */
export const LoginUserResponseSchema: GenMessage<LoginUserResponse> = /*@__PURE__*/
  messageDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 3);

/**
 * Token validation request
 *
 * @generated from message org.example.voicingbackend.audiomodel.ValidateTokenRequest
 */
export type ValidateTokenRequest = Message<"org.example.voicingbackend.audiomodel.ValidateTokenRequest"> & {
  /**
   * @generated from field: string token = 1;
   */
  token: string;
};

/**
 * Describes the message org.example.voicingbackend.audiomodel.ValidateTokenRequest.
 * Use `create(ValidateTokenRequestSchema)` to create a new message.
 */
export const ValidateTokenRequestSchema: GenMessage<ValidateTokenRequest> = /*@__PURE__*/
  messageDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 4);

/**
 * Token validation response
 *
 * @generated from message org.example.voicingbackend.audiomodel.ValidateTokenResponse
 */
export type ValidateTokenResponse = Message<"org.example.voicingbackend.audiomodel.ValidateTokenResponse"> & {
  /**
   * @generated from field: bool valid = 1;
   */
  valid: boolean;

  /**
   * @generated from field: string user_id = 2;
   */
  userId: string;

  /**
   * @generated from field: string username = 3;
   */
  username: string;

  /**
   * @generated from field: string message = 4;
   */
  message: string;
};

/**
 * Describes the message org.example.voicingbackend.audiomodel.ValidateTokenResponse.
 * Use `create(ValidateTokenResponseSchema)` to create a new message.
 */
export const ValidateTokenResponseSchema: GenMessage<ValidateTokenResponse> = /*@__PURE__*/
  messageDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 5);

/**
 * User information
 *
 * @generated from message org.example.voicingbackend.audiomodel.UserInfo
 */
export type UserInfo = Message<"org.example.voicingbackend.audiomodel.UserInfo"> & {
  /**
   * @generated from field: string user_id = 1;
   */
  userId: string;

  /**
   * @generated from field: string username = 2;
   */
  username: string;

  /**
   * @generated from field: string email = 3;
   */
  email: string;

  /**
   * @generated from field: string full_name = 4;
   */
  fullName: string;

  /**
   * @generated from field: int64 created_at = 5;
   */
  createdAt: bigint;

  /**
   * @generated from field: int64 last_login = 6;
   */
  lastLogin: bigint;
};

/**
 * Describes the message org.example.voicingbackend.audiomodel.UserInfo.
 * Use `create(UserInfoSchema)` to create a new message.
 */
export const UserInfoSchema: GenMessage<UserInfo> = /*@__PURE__*/
  messageDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 6);

/**
 * Request to load a model
 *
 * @generated from message org.example.voicingbackend.audiomodel.LoadModelRequest
 */
export type LoadModelRequest = Message<"org.example.voicingbackend.audiomodel.LoadModelRequest"> & {
  /**
   * @generated from field: string model_path = 1;
   */
  modelPath: string;

  /**
   * @generated from field: org.example.voicingbackend.audiomodel.ModelConfig config = 2;
   */
  config?: ModelConfig;
};

/**
 * Describes the message org.example.voicingbackend.audiomodel.LoadModelRequest.
 * Use `create(LoadModelRequestSchema)` to create a new message.
 */
export const LoadModelRequestSchema: GenMessage<LoadModelRequest> = /*@__PURE__*/
  messageDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 7);

/**
 * Response after loading a model
 *
 * @generated from message org.example.voicingbackend.audiomodel.LoadModelResponse
 */
export type LoadModelResponse = Message<"org.example.voicingbackend.audiomodel.LoadModelResponse"> & {
  /**
   * @generated from field: bool success = 1;
   */
  success: boolean;

  /**
   * @generated from field: string message = 2;
   */
  message: string;

  /**
   * @generated from field: org.example.voicingbackend.audiomodel.ModelInfo model_info = 3;
   */
  modelInfo?: ModelInfo;
};

/**
 * Describes the message org.example.voicingbackend.audiomodel.LoadModelResponse.
 * Use `create(LoadModelResponseSchema)` to create a new message.
 */
export const LoadModelResponseSchema: GenMessage<LoadModelResponse> = /*@__PURE__*/
  messageDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 8);

/**
 * Request to process audio data
 *
 * @generated from message org.example.voicingbackend.audiomodel.ProcessAudioRequest
 */
export type ProcessAudioRequest = Message<"org.example.voicingbackend.audiomodel.ProcessAudioRequest"> & {
  /**
   * Raw audio waveform samples
   *
   * @generated from field: repeated float audio_samples = 1;
   */
  audioSamples: number[];

  /**
   * Sample rate (e.g., 16000 for 16kHz)
   *
   * @generated from field: int32 sample_rate = 2;
   */
  sampleRate: number;

  /**
   * @generated from field: int64 timestamp = 3;
   */
  timestamp: bigint;
};

/**
 * Describes the message org.example.voicingbackend.audiomodel.ProcessAudioRequest.
 * Use `create(ProcessAudioRequestSchema)` to create a new message.
 */
export const ProcessAudioRequestSchema: GenMessage<ProcessAudioRequest> = /*@__PURE__*/
  messageDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 9);

/**
 * Response with processed audio data
 *
 * @generated from message org.example.voicingbackend.audiomodel.ProcessAudioResponse
 */
export type ProcessAudioResponse = Message<"org.example.voicingbackend.audiomodel.ProcessAudioResponse"> & {
  /**
   * @generated from field: bool success = 1;
   */
  success: boolean;

  /**
   * Speaker embedding vector (512 dimensions)
   *
   * @generated from field: repeated float embedding = 2;
   */
  embedding: number[];

  /**
   * Confidence score
   *
   * @generated from field: float confidence = 3;
   */
  confidence: number;

  /**
   * @generated from field: string error_message = 4;
   */
  errorMessage: string;
};

/**
 * Describes the message org.example.voicingbackend.audiomodel.ProcessAudioResponse.
 * Use `create(ProcessAudioResponseSchema)` to create a new message.
 */
export const ProcessAudioResponseSchema: GenMessage<ProcessAudioResponse> = /*@__PURE__*/
  messageDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 10);

/**
 * Request to save audio to Google Cloud Storage
 *
 * @generated from message org.example.voicingbackend.audiomodel.SaveAudioRequest
 */
export type SaveAudioRequest = Message<"org.example.voicingbackend.audiomodel.SaveAudioRequest"> & {
  /**
   * Raw audio waveform samples
   *
   * @generated from field: repeated float audio_samples = 1;
   */
  audioSamples: number[];

  /**
   * Sample rate (e.g., 16000 for 16kHz)
   *
   * @generated from field: int32 sample_rate = 2;
   */
  sampleRate: number;

  /**
   * GCS bucket name
   *
   * @generated from field: string bucket_name = 3;
   */
  bucketName: string;

  /**
   * File name in the bucket
   *
   * @generated from field: string file_name = 4;
   */
  fileName: string;

  /**
   * Audio format (WAV, MP3, etc.)
   *
   * @generated from field: org.example.voicingbackend.audiomodel.AudioFormat format = 5;
   */
  format: AudioFormat;

  /**
   * Additional metadata
   *
   * @generated from field: map<string, string> metadata = 6;
   */
  metadata: { [key: string]: string };

  /**
   * @generated from field: int64 timestamp = 7;
   */
  timestamp: bigint;
};

/**
 * Describes the message org.example.voicingbackend.audiomodel.SaveAudioRequest.
 * Use `create(SaveAudioRequestSchema)` to create a new message.
 */
export const SaveAudioRequestSchema: GenMessage<SaveAudioRequest> = /*@__PURE__*/
  messageDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 11);

/**
 * Response after saving audio
 *
 * @generated from message org.example.voicingbackend.audiomodel.SaveAudioResponse
 */
export type SaveAudioResponse = Message<"org.example.voicingbackend.audiomodel.SaveAudioResponse"> & {
  /**
   * @generated from field: bool success = 1;
   */
  success: boolean;

  /**
   * GCS URI of the saved file
   *
   * @generated from field: string gcs_uri = 2;
   */
  gcsUri: string;

  /**
   * File name in the bucket
   *
   * @generated from field: string file_name = 3;
   */
  fileName: string;

  /**
   * Size of the saved file
   *
   * @generated from field: int64 file_size_bytes = 4;
   */
  fileSizeBytes: bigint;

  /**
   * @generated from field: string error_message = 5;
   */
  errorMessage: string;
};

/**
 * Describes the message org.example.voicingbackend.audiomodel.SaveAudioResponse.
 * Use `create(SaveAudioResponseSchema)` to create a new message.
 */
export const SaveAudioResponseSchema: GenMessage<SaveAudioResponse> = /*@__PURE__*/
  messageDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 12);

/**
 * Request to suppress background from audio
 *
 * @generated from message org.example.voicingbackend.audiomodel.SuppressBackgroundRequest
 */
export type SuppressBackgroundRequest = Message<"org.example.voicingbackend.audiomodel.SuppressBackgroundRequest"> & {
  /**
   * Raw audio waveform samples
   *
   * @generated from field: repeated float audio_samples = 1;
   */
  audioSamples: number[];

  /**
   * Sample rate (e.g., 16000)
   *
   * @generated from field: int32 sample_rate = 2;
   */
  sampleRate: number;

  /**
   * Whether to return background estimate
   *
   * @generated from field: bool return_background = 3;
   */
  returnBackground: boolean;

  /**
   * @generated from field: int64 timestamp = 4;
   */
  timestamp: bigint;
};

/**
 * Describes the message org.example.voicingbackend.audiomodel.SuppressBackgroundRequest.
 * Use `create(SuppressBackgroundRequestSchema)` to create a new message.
 */
export const SuppressBackgroundRequestSchema: GenMessage<SuppressBackgroundRequest> = /*@__PURE__*/
  messageDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 13);

/**
 * Response with foreground (speech) and optional background
 *
 * @generated from message org.example.voicingbackend.audiomodel.SuppressBackgroundResponse
 */
export type SuppressBackgroundResponse = Message<"org.example.voicingbackend.audiomodel.SuppressBackgroundResponse"> & {
  /**
   * @generated from field: bool success = 1;
   */
  success: boolean;

  /**
   * Cleaned audio
   *
   * @generated from field: repeated float foreground_samples = 2;
   */
  foregroundSamples: number[];

  /**
   * Estimated background (optional)
   *
   * @generated from field: repeated float background_samples = 3;
   */
  backgroundSamples: number[];

  /**
   * @generated from field: string error_message = 4;
   */
  errorMessage: string;
};

/**
 * Describes the message org.example.voicingbackend.audiomodel.SuppressBackgroundResponse.
 * Use `create(SuppressBackgroundResponseSchema)` to create a new message.
 */
export const SuppressBackgroundResponseSchema: GenMessage<SuppressBackgroundResponse> = /*@__PURE__*/
  messageDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 14);

/**
 * @generated from message org.example.voicingbackend.audiomodel.ExtractEmbeddingsRequest
 */
export type ExtractEmbeddingsRequest = Message<"org.example.voicingbackend.audiomodel.ExtractEmbeddingsRequest"> & {
  /**
   * @generated from field: repeated float audio_samples = 1;
   */
  audioSamples: number[];

  /**
   * @generated from field: int32 sample_rate = 2;
   */
  sampleRate: number;

  /**
   * @generated from field: int64 timestamp = 3;
   */
  timestamp: bigint;

  /**
   * required for storage
   *
   * @generated from field: string user_id = 4;
   */
  userId: string;

  /**
   * optional logical id of the audio/recording
   *
   * @generated from field: string audio_id = 5;
   */
  audioId: string;
};

/**
 * Describes the message org.example.voicingbackend.audiomodel.ExtractEmbeddingsRequest.
 * Use `create(ExtractEmbeddingsRequestSchema)` to create a new message.
 */
export const ExtractEmbeddingsRequestSchema: GenMessage<ExtractEmbeddingsRequest> = /*@__PURE__*/
  messageDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 15);

/**
 * @generated from message org.example.voicingbackend.audiomodel.EmbeddingChunk
 */
export type EmbeddingChunk = Message<"org.example.voicingbackend.audiomodel.EmbeddingChunk"> & {
  /**
   * @generated from field: int64 start_ms = 1;
   */
  startMs: bigint;

  /**
   * @generated from field: int64 end_ms = 2;
   */
  endMs: bigint;

  /**
   * L2-normalized speaker embedding
   *
   * @generated from field: repeated float vector = 3;
   */
  vector: number[];
};

/**
 * Describes the message org.example.voicingbackend.audiomodel.EmbeddingChunk.
 * Use `create(EmbeddingChunkSchema)` to create a new message.
 */
export const EmbeddingChunkSchema: GenMessage<EmbeddingChunk> = /*@__PURE__*/
  messageDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 16);

/**
 * @generated from message org.example.voicingbackend.audiomodel.ExtractEmbeddingsResponse
 */
export type ExtractEmbeddingsResponse = Message<"org.example.voicingbackend.audiomodel.ExtractEmbeddingsResponse"> & {
  /**
   * @generated from field: bool success = 1;
   */
  success: boolean;

  /**
   * @generated from field: repeated org.example.voicingbackend.audiomodel.EmbeddingChunk chunks = 2;
   */
  chunks: EmbeddingChunk[];

  /**
   * @generated from field: string error_message = 3;
   */
  errorMessage: string;
};

/**
 * Describes the message org.example.voicingbackend.audiomodel.ExtractEmbeddingsResponse.
 * Use `create(ExtractEmbeddingsResponseSchema)` to create a new message.
 */
export const ExtractEmbeddingsResponseSchema: GenMessage<ExtractEmbeddingsResponse> = /*@__PURE__*/
  messageDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 17);

/**
 * @generated from message org.example.voicingbackend.audiomodel.VerifyUserRequest
 */
export type VerifyUserRequest = Message<"org.example.voicingbackend.audiomodel.VerifyUserRequest"> & {
  /**
   * user to verify against
   *
   * @generated from field: string user_id = 1;
   */
  userId: string;

  /**
   * audio to verify
   *
   * @generated from field: repeated float audio_samples = 2;
   */
  audioSamples: number[];

  /**
   * @generated from field: int32 sample_rate = 3;
   */
  sampleRate: number;
};

/**
 * Describes the message org.example.voicingbackend.audiomodel.VerifyUserRequest.
 * Use `create(VerifyUserRequestSchema)` to create a new message.
 */
export const VerifyUserRequestSchema: GenMessage<VerifyUserRequest> = /*@__PURE__*/
  messageDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 18);

/**
 * @generated from message org.example.voicingbackend.audiomodel.VerifyUserResponse
 */
export type VerifyUserResponse = Message<"org.example.voicingbackend.audiomodel.VerifyUserResponse"> & {
  /**
   * @generated from field: bool success = 1;
   */
  success: boolean;

  /**
   * cosine similarity in [0,1]
   *
   * @generated from field: float score = 2;
   */
  score: number;

  /**
   * score * 100
   *
   * @generated from field: float percentage = 3;
   */
  percentage: number;

  /**
   * percentage >= threshold
   *
   * @generated from field: bool verified = 4;
   */
  verified: boolean;

  /**
   * @generated from field: string error_message = 5;
   */
  errorMessage: string;
};

/**
 * Describes the message org.example.voicingbackend.audiomodel.VerifyUserResponse.
 * Use `create(VerifyUserResponseSchema)` to create a new message.
 */
export const VerifyUserResponseSchema: GenMessage<VerifyUserResponse> = /*@__PURE__*/
  messageDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 19);

/**
 * @generated from message org.example.voicingbackend.audiomodel.EnrolUserVoiceRequest
 */
export type EnrolUserVoiceRequest = Message<"org.example.voicingbackend.audiomodel.EnrolUserVoiceRequest"> & {
  /**
   * @generated from field: string user_id = 1;
   */
  userId: string;

  /**
   * optional logical id/name
   *
   * @generated from field: string audio_id = 2;
   */
  audioId: string;

  /**
   * @generated from field: repeated float audio_samples = 3;
   */
  audioSamples: number[];

  /**
   * @generated from field: int32 sample_rate = 4;
   */
  sampleRate: number;

  /**
   * default WAV
   *
   * @generated from field: org.example.voicingbackend.audiomodel.AudioFormat store_format = 5;
   */
  storeFormat: AudioFormat;
};

/**
 * Describes the message org.example.voicingbackend.audiomodel.EnrolUserVoiceRequest.
 * Use `create(EnrolUserVoiceRequestSchema)` to create a new message.
 */
export const EnrolUserVoiceRequestSchema: GenMessage<EnrolUserVoiceRequest> = /*@__PURE__*/
  messageDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 20);

/**
 * @generated from message org.example.voicingbackend.audiomodel.EnrolUserVoiceResponse
 */
export type EnrolUserVoiceResponse = Message<"org.example.voicingbackend.audiomodel.EnrolUserVoiceResponse"> & {
  /**
   * @generated from field: bool success = 1;
   */
  success: boolean;

  /**
   * @generated from field: string s3_uri = 2;
   */
  s3Uri: string;

  /**
   * number of embedding windows saved
   *
   * @generated from field: int32 chunks_stored = 3;
   */
  chunksStored: number;

  /**
   * @generated from field: string error_message = 4;
   */
  errorMessage: string;
};

/**
 * Describes the message org.example.voicingbackend.audiomodel.EnrolUserVoiceResponse.
 * Use `create(EnrolUserVoiceResponseSchema)` to create a new message.
 */
export const EnrolUserVoiceResponseSchema: GenMessage<EnrolUserVoiceResponse> = /*@__PURE__*/
  messageDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 21);

/**
 * @generated from message org.example.voicingbackend.audiomodel.TranscribeAudioRequest
 */
export type TranscribeAudioRequest = Message<"org.example.voicingbackend.audiomodel.TranscribeAudioRequest"> & {
  /**
   * Raw file bytes (WAV/MP3/FLAC/OGG)
   *
   * @generated from field: bytes file_content = 1;
   */
  fileContent: Uint8Array;

  /**
   * File name with extension (used for content-type)
   *
   * @generated from field: string file_name = 2;
   */
  fileName: string;

  /**
   * e.g., "whisper-1"; default if empty
   *
   * @generated from field: string model = 3;
   */
  model: string;

  /**
   * optional provider-specific params
   *
   * @generated from field: map<string, string> options = 4;
   */
  options: { [key: string]: string };
};

/**
 * Describes the message org.example.voicingbackend.audiomodel.TranscribeAudioRequest.
 * Use `create(TranscribeAudioRequestSchema)` to create a new message.
 */
export const TranscribeAudioRequestSchema: GenMessage<TranscribeAudioRequest> = /*@__PURE__*/
  messageDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 22);

/**
 * @generated from message org.example.voicingbackend.audiomodel.TranscribeAudioResponse
 */
export type TranscribeAudioResponse = Message<"org.example.voicingbackend.audiomodel.TranscribeAudioResponse"> & {
  /**
   * @generated from field: bool success = 1;
   */
  success: boolean;

  /**
   * @generated from field: string text = 2;
   */
  text: string;

  /**
   * @generated from field: string error_message = 3;
   */
  errorMessage: string;
};

/**
 * Describes the message org.example.voicingbackend.audiomodel.TranscribeAudioResponse.
 * Use `create(TranscribeAudioResponseSchema)` to create a new message.
 */
export const TranscribeAudioResponseSchema: GenMessage<TranscribeAudioResponse> = /*@__PURE__*/
  messageDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 23);

/**
 * @generated from message org.example.voicingbackend.audiomodel.TextToPhonemeRequest
 */
export type TextToPhonemeRequest = Message<"org.example.voicingbackend.audiomodel.TextToPhonemeRequest"> & {
  /**
   * @generated from field: string text = 1;
   */
  text: string;

  /**
   * ISO code like "en"; defaults to config
   *
   * @generated from field: string language = 2;
   */
  language: string;

  /**
   * default ARPABET
   *
   * @generated from field: org.example.voicingbackend.audiomodel.PhonemeFormat format = 3;
   */
  format: PhonemeFormat;

  /**
   * e.g., "en-ng" for Nigerian English
   *
   * @generated from field: string dialect = 4;
   */
  dialect: string;
};

/**
 * Describes the message org.example.voicingbackend.audiomodel.TextToPhonemeRequest.
 * Use `create(TextToPhonemeRequestSchema)` to create a new message.
 */
export const TextToPhonemeRequestSchema: GenMessage<TextToPhonemeRequest> = /*@__PURE__*/
  messageDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 24);

/**
 * @generated from message org.example.voicingbackend.audiomodel.PhonemeSequence
 */
export type PhonemeSequence = Message<"org.example.voicingbackend.audiomodel.PhonemeSequence"> & {
  /**
   * original token/word
   *
   * @generated from field: string token = 1;
   */
  token: string;

  /**
   * phonemes for the token
   *
   * @generated from field: repeated string phonemes = 2;
   */
  phonemes: string[];
};

/**
 * Describes the message org.example.voicingbackend.audiomodel.PhonemeSequence.
 * Use `create(PhonemeSequenceSchema)` to create a new message.
 */
export const PhonemeSequenceSchema: GenMessage<PhonemeSequence> = /*@__PURE__*/
  messageDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 25);

/**
 * @generated from message org.example.voicingbackend.audiomodel.TextToPhonemeResponse
 */
export type TextToPhonemeResponse = Message<"org.example.voicingbackend.audiomodel.TextToPhonemeResponse"> & {
  /**
   * @generated from field: bool success = 1;
   */
  success: boolean;

  /**
   * per-token phoneme sequences
   *
   * @generated from field: repeated org.example.voicingbackend.audiomodel.PhonemeSequence sequences = 2;
   */
  sequences: PhonemeSequence[];

  /**
   * flattened phoneme sequence (joined)
   *
   * @generated from field: repeated string phonemes = 3;
   */
  phonemes: string[];

  /**
   * flattened ids for embeddings
   *
   * @generated from field: repeated int32 phoneme_ids = 5;
   */
  phonemeIds: number[];

  /**
   * @generated from field: string error_message = 4;
   */
  errorMessage: string;
};

/**
 * Describes the message org.example.voicingbackend.audiomodel.TextToPhonemeResponse.
 * Use `create(TextToPhonemeResponseSchema)` to create a new message.
 */
export const TextToPhonemeResponseSchema: GenMessage<TextToPhonemeResponse> = /*@__PURE__*/
  messageDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 26);

/**
 * @generated from message org.example.voicingbackend.audiomodel.TextToSpeechRequest
 */
export type TextToSpeechRequest = Message<"org.example.voicingbackend.audiomodel.TextToSpeechRequest"> & {
  /**
   * @generated from field: string text = 1;
   */
  text: string;

  /**
   * desired output rate; if 0, use default
   *
   * @generated from field: int32 sample_rate = 2;
   */
  sampleRate: number;

  /**
   * e.g., "en-ng" for Nigerian English, "en-us", "en-gb"
   *
   * @generated from field: string voice_style = 3;
   */
  voiceStyle: string;
};

/**
 * Describes the message org.example.voicingbackend.audiomodel.TextToSpeechRequest.
 * Use `create(TextToSpeechRequestSchema)` to create a new message.
 */
export const TextToSpeechRequestSchema: GenMessage<TextToSpeechRequest> = /*@__PURE__*/
  messageDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 27);

/**
 * @generated from message org.example.voicingbackend.audiomodel.TextToSpeechResponse
 */
export type TextToSpeechResponse = Message<"org.example.voicingbackend.audiomodel.TextToSpeechResponse"> & {
  /**
   * @generated from field: bool success = 1;
   */
  success: boolean;

  /**
   * mono waveform [-1,1]
   *
   * @generated from field: repeated float samples = 2;
   */
  samples: number[];

  /**
   * @generated from field: int32 sample_rate = 3;
   */
  sampleRate: number;

  /**
   * @generated from field: string error_message = 4;
   */
  errorMessage: string;
};

/**
 * Describes the message org.example.voicingbackend.audiomodel.TextToSpeechResponse.
 * Use `create(TextToSpeechResponseSchema)` to create a new message.
 */
export const TextToSpeechResponseSchema: GenMessage<TextToSpeechResponse> = /*@__PURE__*/
  messageDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 28);

/**
 * Coherent sentence generation (aim for 20-25 words by default)
 *
 * @generated from message org.example.voicingbackend.audiomodel.GenerateSentenceRequest
 */
export type GenerateSentenceRequest = Message<"org.example.voicingbackend.audiomodel.GenerateSentenceRequest"> & {
  /**
   * default 20
   *
   * @generated from field: int32 min_words = 1;
   */
  minWords: number;

  /**
   * default 25
   *
   * @generated from field: int32 max_words = 2;
   */
  maxWords: number;

  /**
   * optional topic/seed
   *
   * @generated from field: string topic = 3;
   */
  topic: string;
};

/**
 * Describes the message org.example.voicingbackend.audiomodel.GenerateSentenceRequest.
 * Use `create(GenerateSentenceRequestSchema)` to create a new message.
 */
export const GenerateSentenceRequestSchema: GenMessage<GenerateSentenceRequest> = /*@__PURE__*/
  messageDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 29);

/**
 * @generated from message org.example.voicingbackend.audiomodel.GenerateSentenceResponse
 */
export type GenerateSentenceResponse = Message<"org.example.voicingbackend.audiomodel.GenerateSentenceResponse"> & {
  /**
   * @generated from field: bool success = 1;
   */
  success: boolean;

  /**
   * @generated from field: string sentence = 2;
   */
  sentence: string;

  /**
   * @generated from field: int32 word_count = 3;
   */
  wordCount: number;

  /**
   * @generated from field: string error_message = 4;
   */
  errorMessage: string;
};

/**
 * Describes the message org.example.voicingbackend.audiomodel.GenerateSentenceResponse.
 * Use `create(GenerateSentenceResponseSchema)` to create a new message.
 */
export const GenerateSentenceResponseSchema: GenMessage<GenerateSentenceResponse> = /*@__PURE__*/
  messageDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 30);

/**
 * Random word juxtaposition (e.g., 7 words)
 *
 * @generated from message org.example.voicingbackend.audiomodel.GenerateJuxtapositionRequest
 */
export type GenerateJuxtapositionRequest = Message<"org.example.voicingbackend.audiomodel.GenerateJuxtapositionRequest"> & {
  /**
   * default 7
   *
   * @generated from field: int32 num_words = 1;
   */
  numWords: number;
};

/**
 * Describes the message org.example.voicingbackend.audiomodel.GenerateJuxtapositionRequest.
 * Use `create(GenerateJuxtapositionRequestSchema)` to create a new message.
 */
export const GenerateJuxtapositionRequestSchema: GenMessage<GenerateJuxtapositionRequest> = /*@__PURE__*/
  messageDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 31);

/**
 * @generated from message org.example.voicingbackend.audiomodel.GenerateJuxtapositionResponse
 */
export type GenerateJuxtapositionResponse = Message<"org.example.voicingbackend.audiomodel.GenerateJuxtapositionResponse"> & {
  /**
   * @generated from field: bool success = 1;
   */
  success: boolean;

  /**
   * @generated from field: repeated string words = 2;
   */
  words: string[];

  /**
   * @generated from field: string error_message = 3;
   */
  errorMessage: string;
};

/**
 * Describes the message org.example.voicingbackend.audiomodel.GenerateJuxtapositionResponse.
 * Use `create(GenerateJuxtapositionResponseSchema)` to create a new message.
 */
export const GenerateJuxtapositionResponseSchema: GenMessage<GenerateJuxtapositionResponse> = /*@__PURE__*/
  messageDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 32);

/**
 * Request to get model information
 *
 * Empty request
 *
 * @generated from message org.example.voicingbackend.audiomodel.GetModelInfoRequest
 */
export type GetModelInfoRequest = Message<"org.example.voicingbackend.audiomodel.GetModelInfoRequest"> & {
};

/**
 * Describes the message org.example.voicingbackend.audiomodel.GetModelInfoRequest.
 * Use `create(GetModelInfoRequestSchema)` to create a new message.
 */
export const GetModelInfoRequestSchema: GenMessage<GetModelInfoRequest> = /*@__PURE__*/
  messageDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 33);

/**
 * Response with model information
 *
 * @generated from message org.example.voicingbackend.audiomodel.GetModelInfoResponse
 */
export type GetModelInfoResponse = Message<"org.example.voicingbackend.audiomodel.GetModelInfoResponse"> & {
  /**
   * @generated from field: org.example.voicingbackend.audiomodel.ModelInfo model_info = 1;
   */
  modelInfo?: ModelInfo;

  /**
   * @generated from field: bool model_loaded = 2;
   */
  modelLoaded: boolean;
};

/**
 * Describes the message org.example.voicingbackend.audiomodel.GetModelInfoResponse.
 * Use `create(GetModelInfoResponseSchema)` to create a new message.
 */
export const GetModelInfoResponseSchema: GenMessage<GetModelInfoResponse> = /*@__PURE__*/
  messageDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 34);

/**
 * Request to unload model
 *
 * Empty request
 *
 * @generated from message org.example.voicingbackend.audiomodel.UnloadModelRequest
 */
export type UnloadModelRequest = Message<"org.example.voicingbackend.audiomodel.UnloadModelRequest"> & {
};

/**
 * Describes the message org.example.voicingbackend.audiomodel.UnloadModelRequest.
 * Use `create(UnloadModelRequestSchema)` to create a new message.
 */
export const UnloadModelRequestSchema: GenMessage<UnloadModelRequest> = /*@__PURE__*/
  messageDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 35);

/**
 * Response after unloading model
 *
 * @generated from message org.example.voicingbackend.audiomodel.UnloadModelResponse
 */
export type UnloadModelResponse = Message<"org.example.voicingbackend.audiomodel.UnloadModelResponse"> & {
  /**
   * @generated from field: bool success = 1;
   */
  success: boolean;

  /**
   * @generated from field: string message = 2;
   */
  message: string;
};

/**
 * Describes the message org.example.voicingbackend.audiomodel.UnloadModelResponse.
 * Use `create(UnloadModelResponseSchema)` to create a new message.
 */
export const UnloadModelResponseSchema: GenMessage<UnloadModelResponse> = /*@__PURE__*/
  messageDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 36);

/**
 * Configuration for model loading
 *
 * @generated from message org.example.voicingbackend.audiomodel.ModelConfig
 */
export type ModelConfig = Message<"org.example.voicingbackend.audiomodel.ModelConfig"> & {
  /**
   * @generated from field: float confidence_threshold = 1;
   */
  confidenceThreshold: number;

  /**
   * Expected sample rate (e.g., 16000)
   *
   * @generated from field: int32 expected_sample_rate = 2;
   */
  expectedSampleRate: number;

  /**
   * Whether to normalize audio to [-1, 1]
   *
   * @generated from field: bool normalize_audio = 3;
   */
  normalizeAudio: boolean;

  /**
   * @generated from field: map<string, string> additional_params = 4;
   */
  additionalParams: { [key: string]: string };
};

/**
 * Describes the message org.example.voicingbackend.audiomodel.ModelConfig.
 * Use `create(ModelConfigSchema)` to create a new message.
 */
export const ModelConfigSchema: GenMessage<ModelConfig> = /*@__PURE__*/
  messageDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 37);

/**
 * Information about the loaded model
 *
 * @generated from message org.example.voicingbackend.audiomodel.ModelInfo
 */
export type ModelInfo = Message<"org.example.voicingbackend.audiomodel.ModelInfo"> & {
  /**
   * @generated from field: string model_name = 1;
   */
  modelName: string;

  /**
   * @generated from field: string model_version = 2;
   */
  modelVersion: string;

  /**
   * @generated from field: string model_path = 3;
   */
  modelPath: string;

  /**
   * @generated from field: int64 model_size_bytes = 4;
   */
  modelSizeBytes: bigint;

  /**
   * @generated from field: repeated string supported_features = 5;
   */
  supportedFeatures: string[];

  /**
   * @generated from field: org.example.voicingbackend.audiomodel.ModelConfig config = 6;
   */
  config?: ModelConfig;

  /**
   * Embedding dimension (e.g., 512)
   *
   * @generated from field: int32 embedding_dimension = 7;
   */
  embeddingDimension: number;

  /**
   * Input tensor name (e.g., "wave")
   *
   * @generated from field: string input_name = 8;
   */
  inputName: string;

  /**
   * Output tensor name (e.g., "embedding")
   *
   * @generated from field: string output_name = 9;
   */
  outputName: string;
};

/**
 * Describes the message org.example.voicingbackend.audiomodel.ModelInfo.
 * Use `create(ModelInfoSchema)` to create a new message.
 */
export const ModelInfoSchema: GenMessage<ModelInfo> = /*@__PURE__*/
  messageDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 38);

/**
 * Audio format options
 *
 * @generated from enum org.example.voicingbackend.audiomodel.AudioFormat
 */
export enum AudioFormat {
  /**
   * @generated from enum value: WAV = 0;
   */
  WAV = 0,

  /**
   * @generated from enum value: MP3 = 1;
   */
  MP3 = 1,

  /**
   * @generated from enum value: FLAC = 2;
   */
  FLAC = 2,

  /**
   * @generated from enum value: OGG = 3;
   */
  OGG = 3,

  /**
   * @generated from enum value: RAW = 4;
   */
  RAW = 4,
}

/**
 * Describes the enum org.example.voicingbackend.audiomodel.AudioFormat.
 */
export const AudioFormatSchema: GenEnum<AudioFormat> = /*@__PURE__*/
  enumDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 0);

/**
 * Text to phoneme
 *
 * @generated from enum org.example.voicingbackend.audiomodel.PhonemeFormat
 */
export enum PhonemeFormat {
  /**
   * e.g., HH AH L OW
   *
   * @generated from enum value: ARPABET = 0;
   */
  ARPABET = 0,

  /**
   * e.g., həˈloʊ
   *
   * @generated from enum value: IPA = 1;
   */
  IPA = 1,
}

/**
 * Describes the enum org.example.voicingbackend.audiomodel.PhonemeFormat.
 */
export const PhonemeFormatSchema: GenEnum<PhonemeFormat> = /*@__PURE__*/
  enumDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 1);

/**
 * Service for audio model operations (speaker embedding)
 *
 * @generated from service org.example.voicingbackend.audiomodel.AudioModelService
 */
export const AudioModelService: GenService<{
  /**
   * Authentication methods
   *
   * @generated from rpc org.example.voicingbackend.audiomodel.AudioModelService.RegisterUser
   */
  registerUser: {
    methodKind: "unary";
    input: typeof RegisterUserRequestSchema;
    output: typeof RegisterUserResponseSchema;
  },
  /**
   * @generated from rpc org.example.voicingbackend.audiomodel.AudioModelService.LoginUser
   */
  loginUser: {
    methodKind: "unary";
    input: typeof LoginUserRequestSchema;
    output: typeof LoginUserResponseSchema;
  },
  /**
   * @generated from rpc org.example.voicingbackend.audiomodel.AudioModelService.ValidateToken
   */
  validateToken: {
    methodKind: "unary";
    input: typeof ValidateTokenRequestSchema;
    output: typeof ValidateTokenResponseSchema;
  },
  /**
   * Load a PyTorch audio model from a file path
   *
   * @generated from rpc org.example.voicingbackend.audiomodel.AudioModelService.LoadModel
   */
  loadModel: {
    methodKind: "unary";
    input: typeof LoadModelRequestSchema;
    output: typeof LoadModelResponseSchema;
  },
  /**
   * Process audio waveform to extract speaker embedding
   *
   * @generated from rpc org.example.voicingbackend.audiomodel.AudioModelService.ProcessAudio
   */
  processAudio: {
    methodKind: "unary";
    input: typeof ProcessAudioRequestSchema;
    output: typeof ProcessAudioResponseSchema;
  },
  /**
   * Save audio to Google Cloud Storage
   *
   * @generated from rpc org.example.voicingbackend.audiomodel.AudioModelService.SaveAudioToGCS
   */
  saveAudioToGCS: {
    methodKind: "unary";
    input: typeof SaveAudioRequestSchema;
    output: typeof SaveAudioResponseSchema;
  },
  /**
   * Suppress background from incoming audio
   *
   * @generated from rpc org.example.voicingbackend.audiomodel.AudioModelService.SuppressBackground
   */
  suppressBackground: {
    methodKind: "unary";
    input: typeof SuppressBackgroundRequestSchema;
    output: typeof SuppressBackgroundResponseSchema;
  },
  /**
   * Extract embeddings over windows of 500ms with 250ms overlap; VAD filters silence
   *
   * @generated from rpc org.example.voicingbackend.audiomodel.AudioModelService.ExtractEmbeddings
   */
  extractEmbeddings: {
    methodKind: "unary";
    input: typeof ExtractEmbeddingsRequestSchema;
    output: typeof ExtractEmbeddingsResponseSchema;
  },
  /**
   * Enrol a user's voice: store audio in S3 and persist non-silent embeddings
   *
   * @generated from rpc org.example.voicingbackend.audiomodel.AudioModelService.EnrolUserVoice
   */
  enrolUserVoice: {
    methodKind: "unary";
    input: typeof EnrolUserVoiceRequestSchema;
    output: typeof EnrolUserVoiceResponseSchema;
  },
  /**
   * Transcribe an audio file using OpenAI
   *
   * @generated from rpc org.example.voicingbackend.audiomodel.AudioModelService.TranscribeAudio
   */
  transcribeAudio: {
    methodKind: "unary";
    input: typeof TranscribeAudioRequestSchema;
    output: typeof TranscribeAudioResponseSchema;
  },
  /**
   * Convert input text to phonemes
   *
   * @generated from rpc org.example.voicingbackend.audiomodel.AudioModelService.TextToPhoneme
   */
  textToPhoneme: {
    methodKind: "unary";
    input: typeof TextToPhonemeRequestSchema;
    output: typeof TextToPhonemeResponseSchema;
  },
  /**
   * Text to Speech synthesis using FastSpeech2 + HiFiGAN
   *
   * @generated from rpc org.example.voicingbackend.audiomodel.AudioModelService.TextToSpeech
   */
  textToSpeech: {
    methodKind: "unary";
    input: typeof TextToSpeechRequestSchema;
    output: typeof TextToSpeechResponseSchema;
  },
  /**
   * Generate a coherent sentence of target length (20-25 words)
   *
   * @generated from rpc org.example.voicingbackend.audiomodel.AudioModelService.GenerateSentence
   */
  generateSentence: {
    methodKind: "unary";
    input: typeof GenerateSentenceRequestSchema;
    output: typeof GenerateSentenceResponseSchema;
  },
  /**
   * Generate a juxtaposition of N random words (e.g., 7 words)
   *
   * @generated from rpc org.example.voicingbackend.audiomodel.AudioModelService.GenerateJuxtaposition
   */
  generateJuxtaposition: {
    methodKind: "unary";
    input: typeof GenerateJuxtapositionRequestSchema;
    output: typeof GenerateJuxtapositionResponseSchema;
  },
  /**
   * Verify a user's voice; returns similarity percentage and verification flag
   *
   * @generated from rpc org.example.voicingbackend.audiomodel.AudioModelService.VerifyUser
   */
  verifyUser: {
    methodKind: "unary";
    input: typeof VerifyUserRequestSchema;
    output: typeof VerifyUserResponseSchema;
  },
  /**
   * Get model information
   *
   * @generated from rpc org.example.voicingbackend.audiomodel.AudioModelService.GetModelInfo
   */
  getModelInfo: {
    methodKind: "unary";
    input: typeof GetModelInfoRequestSchema;
    output: typeof GetModelInfoResponseSchema;
  },
  /**
   * Unload the current model
   *
   * @generated from rpc org.example.voicingbackend.audiomodel.AudioModelService.UnloadModel
   */
  unloadModel: {
    methodKind: "unary";
    input: typeof UnloadModelRequestSchema;
    output: typeof UnloadModelResponseSchema;
  },
}> = /*@__PURE__*/
  serviceDesc(file_org_example_voicingbackend_audiomodel_audiomodel, 0);

